\documentclass[../draft_1.tex]{subfiles}

\begin{document}

\chapter{Multigrid Methods}

\section{Core Ideas}
%%% MAINLY TAKEN FROM BRIGGS CHAPTER 3
Multigrid Methods are an important class of algorithms to iteratively approximate linear systems of equations of the form $Au = f$ where $A \in \mathbb{R}^n$ is usually a sparse symmetric positive definite matrix, that arises from the discretisation of a differential equation. They were first developed in the early 1960s by the soviet mathematicians Fedorenko and Bachlwalow but only became more well-known and \textit{developed} in the late 1970s through the works of Hackbusch \cite{hackbusch2013multi} and Brandt \cite{brandt1977multi}. The core idea behind them is to approximate the error between an estimate and the unknown actual solution "on different frequencies". Zerlegung. Due to the structure of the matrix of discretised differential equations we only have a local transport of information, therefore simple iterative solvers such as a Jacobi or Gauss-Seidel method damp high frequency components of the error fastest, which leads to a much smoother, lower frequency error after only a few iterations. However the low frequency error on the fine grid becomes high frequency error on the coarse grid and can therefore be damped there more efficiently also applying a smoother there. That is the usage of coarser grids accelarates the transport of low frequency error across the discretised domain. Additionally the coarser grids have significantly less degrees of freedom which makes them computationally much cheaper to handle, until we can solve the problem directly on the coarsest grid. The key is to then restrict and interpolate effectively between the series of nested grids to approximate the error the current and the actual solution. 
\smallskip
\\
Through this combination of coarse grid correction and smoothing one obtains a fast, mesh-size independent convergence rate \textit{for elliptic problems} [source].
\smallskip
\\
SMOOTHER
\smallskip
\\
COARSENING STRATEGIES, cite that $\lambda$ parameter paper
\smallskip
\\
INTERPOLATION AND RESTRICTION OPERATOR
\smallskip
\\
which allows us to consider the solution $u$, the approximate solution $w$ as well as the difference between the two, the error $ e = u - w$ as a linear combination of this basis. Furthermore ... frequency ... eigenfunctions ... eigenvectors. Therefore it is possible to differentiate between high and low frequency error where one generally assumes ... (further discussion of what high and low frequency is later ... physical connection to solution, AMG $ \iff $ GMG )



nested meshes

elliptic problems


In geometric multigrid we have a predefined sequence of nested meshes of specific coarsening factors that can vary in different directions and on different levels. The coarse level spaces still represent the original problem just at a lower resolution. 

Algebraic multigrid on the other hand does not have predefined coarse level spaces but instead they are chosen according to a given rule, that takes the values of $A$ (or other known properties of the problem) into account. This is favorable if .... but also more expensive to compute. 


\section{Basic Algorithm} 

%%%% ALSO BRIGGS P. 40, made more general 

Hence we need the following main ingredients to construct a multigrid method with $l+1$ levels, where $l+1$ represents the finest grid; a smoother $S$, a set of interpolation and restriction matrices $I_l, I_{l-1}, ...I_1$ and $R_l, R_{l-1}, ... R_1$, represenations of the matrix $A$ on all levels, that is $A^l, A^{l-1}, A^{l-2}, ..., A^0$ with $A_l = A$, an initial guess $s_{\text{init}}$. These we can then put together in a multigrid V-cycle who looks as follows. 

\begin{framed}
	\underline{\textbf{Multigrid V-cycle}} 
	\smallskip 
	\\
	Let $w^J$ be the initial guess (on the finest grid level). Then repeat the following until convergence criterium is met or number of iterations exceeds a certain threshold:
	
	\begin{itemize}
		\item do $\nu_{J_a}$ smoothing steps on $A^J u^J = f^J$ with initial guess $w^J$
		\item compute $f^{J-1} = I_{J}^{J-1} r^J$
		\item do $\nu_{J-1_a}$ smoothing steps on $A^{J-1} u^{J-1} = f^{J-1}$ with initial guess $w^{J-1} = 0$ (vector)
		\item compute $f^{J-2} = I_{J-1}^{J-2} r^{J-1}$
		\item do $\nu_{J-2_a}$ smoothing steps on $A^{J-2} u^{J-2} = f^{J-2}$ with initial guess $w^{J-2} = 0$ (vector)
		\item compute $f^{J-3} = I_{J-2}^{J-3} r^{J-2}$
		
		... \\
		...
		
		\item solve $A^0 u^0 = f^0$
		
		... \\
		...
		
		\item correct $w^{J-2} = w^{J-2} + I_{J-3}^{J-2} w^{J-3}$
		\item do $\nu_{J-2_b}$ smoothing steps on $A^{J-2} u^{J-2} = f^{J-2}$ with initial guess $w^{J-2}$
		\item correct $w^{J-1} = w^{J-1} + I_{J-2}^{J-1} w^{J-2}$
		\item do $\nu_{{J-1}_b}$ smoothing steps on $A^{J-1} u^{J-1} = f^{J-1}$ with initial guess $w^{J-1}$		
		\item correct $w^{J} = w^{J} + I_{J-1}^{J} w^{J-1}$
		\item do $\nu_{J_b}$ smoothing steps on $A^{J} u^{J} = f^{J}$ with initial guess $w^{J}$
	\end{itemize}	
\end{framed}


picture V-cycle?



\section{Convergence Properties and Complexity}

strengthened Cauchy - Schwarz necessary?


\begin{Theorem}
	Convergence.
\end{Theorem}

%\section{Algebraic Multigrid}
\section{Coarse Space Construction Based on Eigenfunctions}

%wikipedia
Practically important extensions of multigrid methods include techniques where no partial differential equation nor geometrical problem background is used to construct the multilevel hierarchy.[16] Such algebraic multigrid methods (AMG) construct their hierarchy of operators directly from the system matrix. In classical AMG, the levels of the hierarchy are simply subsets of unknowns without any geometric interpretation.
\smallskip
\\
\begin{Definition}
	Strong dependence.
\end{Definition}

As briefly mentioned before 

Concept of strong dependence. Different ways to define this, most commonly ...? 

This is where later on the adaption to the monodomain equation is made because here we can take specific knowledge about the equation into consideration. 




\end{document}