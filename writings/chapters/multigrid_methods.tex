\documentclass[../draft_1.tex]{subfiles}

\begin{document}

\chapter{Multigrid Methods}

\section{Core Ideas}
%%% MAINLY TAKEN FROM BRIGGS CHAPTER 3
Multigrid Methods are an important class of algorithms to iteratively approximate linear systems of equations of the form
 \begin{ceqn}
 	\begin{align}
 	\label{lin_system}
 Au = f 
 	\end{align}
\end{ceqn}

 where $A \in \mathbb{R}^{m \times m}$ is usually a sparse symmetric positive definite matrix, that arises from the discretisation of a differential equation on a discretised domain $\Omega$. They were first developed in the early 1960s by the soviet mathematicians Fedorenko and Bachlwalow but only became more well-known and further developed in the late 1970s through the works of Hackbusch \cite{hackbusch2013multi} and Brandt \cite{brandt1977multi}.
\smallskip
\\
\textbf{Remark:} Here $u$ now denotes the solution to a discretised problem, and has in previous chapters been called $u^h$ as from now on we will only be dealing with the finite dimensional approximation, and therefore for brevity we will drop the superscript $h$, as we will now have iterative solutions denoted by $k$. Similarly $\Omega$ now denotes the discretised domain.
\smallskip
\\
There are different ways to motivate the construction of multigrid algorithms. One of them is to start from an iteration built on approximating solutions of the residual equation. That is for any approximation $u^k$ of (\ref{lin_system}), we denote the error $e^k$, the residual or defect $r^k$ and the overall iteration called defect equation by
\begin{ceqn}
	\begin{align}
	e^k :=& u - u^k \\
	r^k :=& f - Au^k \\
	\label{defect_equation}
	A e^k =& \ r^k	
	\end{align}
\end{ceqn}
For now we have only rewritten equation (\ref{lin_system}), subtracting the term $Au^k$ on either side, as $A e^k = A( u - u^k) = f - Au^k$. However if $A$ is approximated by a "simpler" invertible operator $\hat{A}$, then the solution of 
\begin{ceqn}
	\begin{align}
	\label{simple_system}
\hat{A} \hat{e}^k = r^k
	\end{align}
\end{ceqn}
gives us a new approximation 
\begin{ceqn}
	\begin{align}
u^{k+1} := u^k + \hat{e}^k.
	\end{align}
\end{ceqn}
That is we have a current iterate $u^k$, compute the residual $r^k$, solve the simpler system (\ref{simple_system}) and can hence compute a new iterate $u^{k+1}$. If we start from some initial iterate $u^0$, the consecutive application leads to an iterative procedure, with the iteration operator $M$ and the error iteration 
\begin{ceqn}
	\begin{align}
&M = I - (\hat{A})^{-1} A \\
&e^{k+1} = M e^k
	\end{align}
\end{ceqn}
where $I$ stands for the identity operator. Hence the error reduction can be bounded from above by $\rho := \| M \|$ for one iteration, where it is worth noting that we can only be sure to converge at all if $\rho < 1$.  The question that arises now is how to construct an operator $\hat{A}$, that leads to a value of $\rho$ that is as small as possible and still simple to invert. 
\smallskip 
\\
simple iteration scheme. 
\smallskip
\\
One possibility to solve the defect equation approximately is to construct an approximation $A_C$ of $A$ on a coarser grid $\Omega_C$, such 
\smallskip
\\
\textit{need to somehow introduce what they call $\mathcal{G}(\Omega)$}
\smallskip
\\
Coarse grid correction by itself not effective as .... 
\smallskip
\\
The core idea behind them is to approximate the error between an estimate and the unknown actual solution "on different frequencies". Zerlegung. Due to the structure of the matrix of discretised differential equations we only have a local transport of information, therefore simple iterative solvers such as a Jacobi or Gauss-Seidel method damp high frequency components of the error fastest, which leads to a much smoother, lower frequency error after only a few iterations. However the low frequency error on the fine grid becomes high frequency error on the coarse grid and can therefore be damped there more efficiently also applying a smoother there. That is the usage of coarser grids accelarates the transport of low frequency error across the discretised domain. Additionally the coarser grids have significantly less degrees of freedom which makes them computationally much cheaper to handle, until we can solve the problem directly on the coarsest grid. The key is to then restrict and interpolate effectively between the series of nested grids to approximate the error of the current and the actual solution. 
\smallskip
\\
Through this combination of coarse grid correction and smoothing one obtains a fast, mesh-size independent convergence rate \textit{for elliptic problems} [source].
\smallskip
\\
SMOOTHER
\smallskip
\\
COARSENING STRATEGIES, cite that $\lambda$ parameter paper
\smallskip
\\
INTERPOLATION AND RESTRICTION OPERATOR
\smallskip
\\
which allows us to consider the solution $u$, the approximate solution $w$ as well as the difference between the two, the error $ e = u - w$ as a linear combination of this basis. Furthermore ... frequency ... eigenfunctions ... eigenvectors. Therefore it is possible to differentiate between high and low frequency error where one generally assumes ... (further discussion of what high and low frequency is later ... physical connection to solution, AMG $ \iff $ GMG )



nested meshes

elliptic problems


In geometric multigrid we have a predefined sequence of nested meshes of specific coarsening factors that can vary in different directions and on different levels. The coarse level spaces still represent the original problem just at a lower resolution. 

Algebraic multigrid on the other hand does not have predefined coarse level spaces but instead they are chosen according to a given rule, that takes the values of $A$ (or other known properties of the problem) into account. This is favorable if .... but also more expensive to compute. 


\section{Basic Algorithm} 

%%%% ALSO BRIGGS P. 40, made more general 

Hence we need the following main ingredients to construct a multigrid method with $l+1$ levels, where $l+1$ represents the finest grid; a smoother $S$, a set of interpolation and restriction matrices $I_l, I_{l-1}, ...I_1$ and $R_l, R_{l-1}, ... R_1$, represenations of the matrix $A$ on all levels, that is $A^l, A^{l-1}, A^{l-2}, ..., A^0$ with $A_l = A$, an initial guess $s_{\text{init}}$. These we can then put together in a multigrid V-cycle who looks as follows. 

\begin{framed}
	\underline{\textbf{Multigrid V-cycle}} 
	\smallskip 
	\\
	Let $w^J$ be the initial guess (on the finest grid level). Then repeat the following until convergence criterium is met or number of iterations exceeds a certain threshold:
	
	\begin{itemize}
		\item do $\nu_{J_a}$ smoothing steps on $A^J u^J = f^J$ with initial guess $w^J$
		\item compute $f^{J-1} = I_{J}^{J-1} r^J$
		\begin{itemize}
			\item do $\nu_{J-1_a}$ smoothing steps on $A^{J-1} u^{J-1} = f^{J-1}$ with initial guess $w^{J-1} = 0$ (vector)
			\item compute $f^{J-2} = I_{J-1}^{J-2} r^{J-1}$
			\begin{itemize}
				\item do $\nu_{J-2_a}$ smoothing steps on $A^{J-2} u^{J-2} = f^{J-2}$ with initial guess $w^{J-2} = 0$ (vector)
				\item compute $f^{J-3} = I_{J-2}^{J-3} r^{J-2}$
		
		... \\
		...
				\begin{itemize}
				\item solve $A^0 u^0 = f^0$
				\end{itemize}	
		... \\
		...
		
			\item correct $w^{J-2} = w^{J-2} + I_{J-3}^{J-2} w^{J-3}$
			\item do $\nu_{J-2_b}$ smoothing steps on $A^{J-2} u^{J-2} = f^{J-2}$ with initial guess $w^{J-2}$
			\end{itemize}	
		\item correct $w^{J-1} = w^{J-1} + I_{J-2}^{J-1} w^{J-2}$
		\item do $\nu_{{J-1}_b}$ smoothing steps on $A^{J-1} u^{J-1} = f^{J-1}$ with initial guess $w^{J-1}$		
			\end{itemize}	
		\item correct $w^{J} = w^{J} + I_{J-1}^{J} w^{J-1}$
		\item do $\nu_{J_b}$ smoothing steps on $A^{J} u^{J} = f^{J}$ with initial guess $w^{J}$
	\end{itemize}	
\end{framed}

\section{Convergence Properties and Complexity}

strengthened Cauchy - Schwarz necessary?


\begin{Theorem}
	Convergence.
\end{Theorem}

%\section{Algebraic Multigrid}
\section{Coarse Space Construction Based on Eigenfunctions}

%wikipedia
Practically important extensions of multigrid methods include techniques where no partial differential equation nor geometrical problem background is used to construct the multilevel hierarchy.[16] Such algebraic multigrid methods (AMG) construct their hierarchy of operators directly from the system matrix. In classical AMG, the levels of the hierarchy are simply subsets of unknowns without any geometric interpretation.
\smallskip
\\
\begin{Definition}
	Strong dependence.
\end{Definition}

As briefly mentioned before 

Concept of strong dependence. Different ways to define this, most commonly ...? 

This is where later on the adaption to the monodomain equation is made because here we can take specific knowledge about the equation into consideration. 




\end{document}